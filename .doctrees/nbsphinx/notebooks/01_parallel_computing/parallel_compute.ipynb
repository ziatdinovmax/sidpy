{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:51.421037Z",
     "iopub.status.busy": "2020-10-03T00:35:51.420190Z",
     "iopub.status.idle": "2020-10-03T00:35:51.686610Z",
     "shell.execute_reply": "2020-10-03T00:35:51.687037Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Speed up computations with parallel_compute()\n",
    "\n",
    "**Suhas Somnath, Chris R. Smith**\n",
    "\n",
    "9/8/2017\n",
    "\n",
    "**This document will demonstrate how ``sidpy.proc.comp_utils.parallel_compute()`` can significantly speed up data processing by\n",
    "using all available CPU cores in a computer**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Quite often, we need to perform the same operation on every single component in our data. One of the most popular\n",
    "examples is functional fitting applied to spectra collected at each location on a grid. While, the operation itself\n",
    "may not take very long, computing this operation thousands of times, once per location, using a single CPU core can\n",
    "take a long time to complete. Most personal computers today come with at least two cores, and in many cases, each of\n",
    "these cores is represented via two logical cores, thereby summing to a total of at least four cores. Thus, it is\n",
    "prudent to make use of these unused cores whenever possible. Fortunately, there are a few python packages that\n",
    "facilitate the efficient use of all CPU cores with minimal modifications to the existing code.\n",
    "\n",
    "``sidpy.proc.comp_utils.parallel_compute()`` is a very handy function that simplifies parallel computation significantly to a\n",
    "**single function call** and will be discussed in this document.\n",
    "\n",
    "## Example scientific problem\n",
    "For this example, we will be working with a ``Band Excitation Piezoresponse Force Microscopy (BE-PFM)`` imaging dataset\n",
    "acquired from advanced atomic force microscopes. In this dataset, a spectra was collected for each position in a two\n",
    "dimensional grid of spatial locations. Thus, this is a three dimensional dataset that has been flattened to a two\n",
    "dimensional matrix in accordance with **Universal Spectroscopy and Imaging Data (USID)** model.\n",
    "\n",
    "Each spectra in this dataset is expected to have a single peak. The goal is to find the positions of the peaks in each\n",
    "spectra. Clearly, the operation of finding the peak in one spectra is independent of the same operation on another\n",
    "spectra. Thus, we could in theory divide the dataset in to N parts and use N CPU cores to compute the results much\n",
    "faster than it would take a single core to compute the results. There is an important caveat to this statement and it\n",
    "will be discussed at the end of this document.\n",
    "\n",
    "**Here, we will learn how to fit the thousands of spectra using all available cores on a computer.**\n",
    "Note, that this is applicable only for a single CPU. Please refer to another advanced example for multi-CPU computing.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to run this document on your own computer, you will need to:\n",
    "\n",
    "    1. Download the document as a Jupyter notebook using the link at the bottom of this page.\n",
    "    2. Save the contents of `this python file <https://github.com/pycroscopy/sidpy/blob/master/examples/proc/supporting_docs/peak_finding.py>`_ as ``peak_finding.py`` in the\n",
    "       same folder as the notebook from step 1.</p></div>\n",
    "\n",
    "Ensure python 3 compatibility:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:51.720506Z",
     "iopub.status.busy": "2020-10-03T00:35:51.693733Z",
     "iopub.status.idle": "2020-10-03T00:35:52.061964Z",
     "shell.execute_reply": "2020-10-03T00:35:52.061471Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peak_finding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c04ac15a58a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./supporting_docs/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpeak_finding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_all_peaks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peak_finding'"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import, unicode_literals\n",
    "\n",
    "# The package for accessing files in directories, etc.:\n",
    "import os\n",
    "\n",
    "# Warning package in case something goes wrong\n",
    "from warnings import warn\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def install(package):\n",
    "    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "# Package for downloading online files:\n",
    "try:\n",
    "    # This package is not part of anaconda and may need to be installed.\n",
    "    import wget\n",
    "except ImportError:\n",
    "    warn('wget not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install(wget)\n",
    "    import wget\n",
    "\n",
    "# The mathematical computation package:\n",
    "import numpy as np\n",
    "\n",
    "# The package used for creating and manipulating HDF5 files:\n",
    "import h5py\n",
    "\n",
    "# Packages for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parallel computation library:\n",
    "try:\n",
    "    import joblib\n",
    "except ImportError:\n",
    "    warn('joblib not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install('joblib')\n",
    "    import joblib\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# A handy python utility that allows us to preconfigure parts of a function\n",
    "from functools import partial\n",
    "\n",
    "# Finally import sidpy:\n",
    "try:\n",
    "    from sidpy.proc.comp_utils import parallel_compute\n",
    "except ImportError:\n",
    "    warn('sidpy not found.  Will install with pip.')\n",
    "    import pip\n",
    "    install('sidpy')\n",
    "    from sidpy.proc.comp_utils import parallel_compute\n",
    "\n",
    "# import the scientific function:\n",
    "import sys\n",
    "sys.path.append('./supporting_docs/')\n",
    "from peak_finding import find_all_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "In order to demonstrate parallel computing, we will be using a real experimental dataset that is available on the\n",
    "pyUSID GitHub project. First, lets download this file from Github:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.068073Z",
     "iopub.status.busy": "2020-10-03T00:35:52.066067Z",
     "iopub.status.idle": "2020-10-03T00:35:52.653283Z",
     "shell.execute_reply": "2020-10-03T00:35:52.653740Z"
    }
   },
   "outputs": [],
   "source": [
    "h5_path = 'temp.h5'\n",
    "url = 'https://raw.githubusercontent.com/pycroscopy/pyUSID/master/data/BELine_0004.h5'\n",
    "if os.path.exists(h5_path):\n",
    "    os.remove(h5_path)\n",
    "_ = wget.download(url, h5_path, bar=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets open this HDF5 file. The focus of this example is not on the data storage or arrangement but rather on\n",
    "demonstrating parallel computation so lets dive straight into the main dataset that requires fitting of the spectra:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.660228Z",
     "iopub.status.busy": "2020-10-03T00:35:52.659694Z",
     "iopub.status.idle": "2020-10-03T00:35:52.663957Z",
     "shell.execute_reply": "2020-10-03T00:35:52.663492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The main dataset:\n",
      "------------------------------------\n",
      "<HDF5 dataset \"Raw_Data\": shape (16384, 119), type \"<c8\">\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read-only mode\n",
    "h5_file = h5py.File(h5_path, mode='r')\n",
    "# Get handle to the the raw data\n",
    "h5_meas_grp = h5_file['Measurement_000']\n",
    "\n",
    "# Accessing the dataset of interest:\n",
    "h5_main = h5_meas_grp['Channel_000/Raw_Data']\n",
    "print('\\nThe main dataset:\\n------------------------------------')\n",
    "print(h5_main)\n",
    "\n",
    "num_cols = 128\n",
    "cores_vec = list()\n",
    "times_vec = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The operation\n",
    "The scipy package has a very handy function called *find_peaks_cwt()* that facilitates the search for one or more\n",
    "peaks in a spectrum. We will be using a function called *find_all_peaks()* that uses *find_peaks_cwt()*.\n",
    "For the purposes of this example, we do not be concerned with how this\n",
    "function works. All we need to know is that this function takes 3 inputs:\n",
    "\n",
    "* ``vector`` - a 1D array containing the spectra at a single location\n",
    "* ``width_bounds`` - something like [20, 50] that instructs the function to look for peaks that are 20-50\n",
    "  data-points wide. The function will look for a peak with width of 20, then again for a peak of width - 21 and so on.\n",
    "* ``num_steps`` - The number of steps within the possible widths [20, 50], that the search must be performed\n",
    "\n",
    "The function has one output:\n",
    "\n",
    "* ``peak_indices`` - an array of the positions at which peaks were found.\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "   def find_all_peaks(vector, width_bounds, num_steps=20, **kwargs):\n",
    "       \"\"\"\n",
    "       This is the function that will be mapped by multiprocess. This is a wrapper around the scipy function.\n",
    "       It uses a parameter - wavelet_widths that is configured outside this function.\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       vector : 1D numpy array\n",
    "           Feature vector containing peaks\n",
    "       width_bounds : tuple / list / iterable\n",
    "           Min and max for the size of the window\n",
    "       num_steps : uint, (optional). Default = 20\n",
    "           Number of different peak widths to search\n",
    "\n",
    "       Returns\n",
    "       -------\n",
    "       peak_indices : list\n",
    "           List of indices of peaks within the prescribed peak widths\n",
    "       \"\"\"\n",
    "       # The below numpy array is used to configure the returned function wpeaks\n",
    "       wavelet_widths = np.linspace(width_bounds[0], width_bounds[1], num_steps)\n",
    "\n",
    "       peak_indices = find_peaks_cwt(np.abs(vector), wavelet_widths, **kwargs)\n",
    "\n",
    "       return peak_indices\n",
    "\n",
    "## Testing the function\n",
    "Let’s see what the operation on an example spectra returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.672110Z",
     "iopub.status.busy": "2020-10-03T00:35:52.671347Z",
     "iopub.status.idle": "2020-10-03T00:35:52.678250Z",
     "shell.execute_reply": "2020-10-03T00:35:52.677824Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_all_peaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bcc6021ab475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspectra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpixel_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpeak_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_all_peaks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_all_peaks' is not defined"
     ]
    }
   ],
   "source": [
    "row_ind, col_ind = 103, 19\n",
    "pixel_ind = col_ind + row_ind * num_cols\n",
    "spectra = h5_main[pixel_ind]\n",
    "\n",
    "peak_inds = find_all_peaks(spectra, [20, 60], num_steps=30)\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.scatter(np.arange(len(spectra)), np.abs(spectra), c='black')\n",
    "axis.axvline(peak_inds[0], color='r', linewidth=2)\n",
    "axis.set_ylim([0, 1.1 * np.max(np.abs(spectra))]);\n",
    "axis.set_title('find_all_peaks found peaks at index: {}'.format(peak_inds), fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply the function to the entire dataset, lets load the dataset to memory so that file-loading time is not a\n",
    "factor when comparing the times for serial and parallel computing times:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.796118Z",
     "iopub.status.busy": "2020-10-03T00:35:52.794752Z",
     "iopub.status.idle": "2020-10-03T00:35:52.797181Z",
     "shell.execute_reply": "2020-10-03T00:35:52.797570Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data = h5_main[()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>This documentation is being generated automatically by a computer in the cloud whose workload cannot be controlled\n",
    "    or predicted. Therefore, the computational times reported in this document may not be consistent and can even be\n",
    "    contradictory. For best results, we recommend that download and run this document as a jupyter notebook.</p></div>\n",
    "\n",
    "## Serial computing\n",
    "A single call to the function does not take substantial time. However, performing the same operation on each of the\n",
    "``16,384`` pixels sequentially can take substantial time. The simplest way to find all peak positions is to simply loop\n",
    "over each position in the dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.808415Z",
     "iopub.status.busy": "2020-10-03T00:35:52.807767Z",
     "iopub.status.idle": "2020-10-03T00:35:52.810718Z",
     "shell.execute_reply": "2020-10-03T00:35:52.811163Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_all_peaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3d6dd23c2ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mserial_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_all_peaks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtimes_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcores_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_all_peaks' is not defined"
     ]
    }
   ],
   "source": [
    "serial_results = list()\n",
    "\n",
    "t_0 = time.time()\n",
    "for vector in raw_data:\n",
    "    serial_results.append(find_all_peaks(vector, [20, 60], num_steps=30))\n",
    "times_vec.append(time.time()-t_0)\n",
    "cores_vec.append(1)\n",
    "print('Serial computation took', np.round(times_vec[-1], 2), ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sidpy.proc.comp_utils.parallel_compute()\n",
    "\n",
    "There are several libraries that can utilize multiple CPU cores to perform the same computation in parallel. Popular\n",
    "examples are ``Multiprocessing``, ``Mutiprocess``, ``Dask``, ``Joblib`` etc. Each of these has their own\n",
    "strengths and weaknesses. Some of them have painful caveats such as the inability to perform the parallel computation\n",
    "within a jupyter notebook. In order to lower the barrier to parallel computation, we have developed a very handy\n",
    "function called ``sidpy.proc.comp_utils.parallel_compute()`` that simplifies the process to a single function call.\n",
    "\n",
    "It is a lot **more straightforward** to provide the arguments and keyword arguments of the function that needs to be\n",
    "applied to the entire dataset. Furthermore, this function intelligently assigns the number of CPU cores for the\n",
    "parallel computation based on the size of the dataset and the computational complexity of the unit computation.\n",
    "For instance, it scales down the number of cores for small datasets if each computation is short. It also ensures that\n",
    "1-2 cores fewer than all available cores are used by default so that the user can continue using their computer for\n",
    "other purposes while the computation runs.\n",
    "\n",
    "Lets apply this ``parallel_compute`` to this problem:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.821012Z",
     "iopub.status.busy": "2020-10-03T00:35:52.820364Z",
     "iopub.status.idle": "2020-10-03T00:35:52.823427Z",
     "shell.execute_reply": "2020-10-03T00:35:52.823930Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_all_peaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ad7c0f471083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Execute the parallel computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m parallel_results = parallel_compute(raw_data, find_all_peaks,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                          \u001b[0mcores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_cores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                          \u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_all_peaks' is not defined"
     ]
    }
   ],
   "source": [
    "cpu_cores = 2\n",
    "args = [[20, 60]]\n",
    "kwargs = {'num_steps': 30}\n",
    "\n",
    "t_0 = time.time()\n",
    "\n",
    "# Execute the parallel computation\n",
    "parallel_results = parallel_compute(raw_data, find_all_peaks,\n",
    "                                         cores=cpu_cores, func_args=args,\n",
    "                                         func_kwargs=kwargs,\n",
    "                                         joblib_backend='multiprocessing')\n",
    "\n",
    "cores_vec.append(cpu_cores)\n",
    "times_vec.append(time.time()-t_0)\n",
    "print('Parallel computation with {} cores took {} seconds'.format(cpu_cores, np.round(times_vec[-1], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results\n",
    "By comparing the run-times for the two approaches, we see that the parallel computation is substantially faster than\n",
    "the serial computation. Note that the numbers will differ between computers. Also, the computation was performed on\n",
    "a relatively small dataset for illustrative purposes. The benefits of using such parallel computation will be far\n",
    "more apparent for much larger datasets.\n",
    "\n",
    "Let's compare the results from both the serial and parallel methods to ensure they give the same results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.832263Z",
     "iopub.status.busy": "2020-10-03T00:35:52.831615Z",
     "iopub.status.idle": "2020-10-03T00:35:52.834477Z",
     "shell.execute_reply": "2020-10-03T00:35:52.834905Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f95e8e46678b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Result from serial computation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserial_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpixel_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Result from parallel computation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpixel_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print('Result from serial computation: {}'.format(serial_results[pixel_ind]))\n",
    "print('Result from parallel computation: {}'.format(parallel_results[pixel_ind]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying the function\n",
    "Note that the ``width_bounds`` and ``num_steps`` arguments will not be changed from one pixel to another. It would be\n",
    "great if we didn't have to keep track of these constant arguments. We can use a very handy python tool called\n",
    "``partial()`` to do just this. Below, all we are doing is creating a new function that always passes our preferred\n",
    "values for ``width_bounds`` and ``num_steps`` arguments to find_all_peaks. While it may seem like this is unimportant,\n",
    "it is very convenient when setting up the parallel computing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.842415Z",
     "iopub.status.busy": "2020-10-03T00:35:52.838084Z",
     "iopub.status.idle": "2020-10-03T00:35:52.844777Z",
     "shell.execute_reply": "2020-10-03T00:35:52.845204Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_all_peaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2e5d5c2638c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_peaks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_all_peaks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_bounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'find_all_peaks' is not defined"
     ]
    }
   ],
   "source": [
    "find_peaks = partial(find_all_peaks, num_steps=30, width_bounds=[20, 60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that even though ``width_bounds`` is an argument, it needs to be specified as though it were a keyword argument\n",
    "like ``num_steps``.\n",
    "Let's try calling our simplified function, ``find_peaks()`` to make sure that it results in the same peak index for the\n",
    "aforementioned chosen spectra:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.850188Z",
     "iopub.status.busy": "2020-10-03T00:35:52.848536Z",
     "iopub.status.idle": "2020-10-03T00:35:52.855756Z",
     "shell.execute_reply": "2020-10-03T00:35:52.856159Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_peaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c65a00b392bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'find_peaks found peaks at index: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_peaks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5_main\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpixel_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'find_peaks' is not defined"
     ]
    }
   ],
   "source": [
    "print('find_peaks found peaks at index: {}'.format(find_peaks(h5_main[pixel_ind])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More cores!\n",
    "Lets use ``find_peaks()`` instead of ``find_all_peaks`` on the entire dataset but increase the number of cores to 3.\n",
    "Note that we do not need to specify ``func_kwargs`` anymore. Also note that this is a very simple function and the\n",
    "benefits of ``partial()`` will be greater for more complex problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.865998Z",
     "iopub.status.busy": "2020-10-03T00:35:52.865295Z",
     "iopub.status.idle": "2020-10-03T00:35:52.868179Z",
     "shell.execute_reply": "2020-10-03T00:35:52.868599Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_peaks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c25ddf058c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Execute the parallel computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m parallel_results = parallel_compute(raw_data, find_peaks,\n\u001b[0m\u001b[1;32m      7\u001b[0m                                          \u001b[0mcores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_cores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                          joblib_backend='multiprocessing')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_peaks' is not defined"
     ]
    }
   ],
   "source": [
    "cpu_cores = 3\n",
    "\n",
    "t_0 = time.time()\n",
    "\n",
    "# Execute the parallel computation\n",
    "parallel_results = parallel_compute(raw_data, find_peaks,\n",
    "                                         cores=cpu_cores,\n",
    "                                         joblib_backend='multiprocessing')\n",
    "\n",
    "cores_vec.append(cpu_cores)\n",
    "times_vec.append(time.time()-t_0)\n",
    "print('Parallel computation with {} cores took {} seconds'.format(cpu_cores, np.round(times_vec[-1], 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalability\n",
    "Now lets see how the computational time relates to the number of cores.\n",
    "Depending on your computer (and what was running on your computer along with this computation), you are likely to see\n",
    "diminishing benefits of additional cores beyond 2 cores for this specific problem in the plot below. This is because\n",
    "the dataset is relatively small and each peak-finding operation is relatively quick. The overhead of adding additional\n",
    "cores quickly outweighs the speedup in distributing the work among multiple CPU cores.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>This documentation is being generated automatically by a computer in the cloud whose workload cannot be controlled\n",
    "    or predicted. Therefore, the computational times reported in this document may not be consistent and can even be\n",
    "    contradictory. For best results, we recommend that download and run this document as a jupyter notebook.\n",
    "\n",
    "    If everything ran correctly, you should see the computational time decrease substantially from 1 to 2 cores but\n",
    "    the decrease from 2 to 3 or 3 to 4 cores should be minimal or negligible.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:52.884370Z",
     "iopub.status.busy": "2020-10-03T00:35:52.879097Z",
     "iopub.status.idle": "2020-10-03T00:35:53.061053Z",
     "shell.execute_reply": "2020-10-03T00:35:53.060606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 252x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axis = plt.subplots(figsize=(3.5, 3.5))\n",
    "axis.scatter(cores_vec, times_vec)\n",
    "axis.set_xlabel('CPU cores', fontsize=14)\n",
    "axis.set_ylabel('Compute time (sec)', fontsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practices for parallel computing\n",
    " --------------------------------------\n",
    "\n",
    " While it may seem tempting to do everything in parallel, it is important to be aware of some of the trade-offs and\n",
    " best-practices for parallel computing (multiple CPU cores) when compared to traditional serial computing (single\n",
    " CPU core):\n",
    "\n",
    " * There is noticeable time overhead involved with setting up each compute worker (CPU core in this case).\n",
    "   For very simple or small computations, this overhead may outweigh the speed-up gained with using multiple cores.\n",
    " * Parallelizing computations that read and write to files at each iteration may be actually be noticeably *slower*\n",
    "   than serial computation since the cores will compete for rights to read and write to the file(s)\n",
    "   and these input/output operations are by far the slowest components of the workflow. Instead, it makes sense to\n",
    "   read large amounts of data from the necessary files once, perform the computation, and then write to the files once\n",
    "   after all the computation is complete. In fact, this is what we automatically do in the ``Process`` class in pyUSID or pyNSID.\n",
    "   Please see `another example <./plot_process.html>`_ on how to write a Process class to formalize data processing.\n",
    "\n",
    " .. note::\n",
    "     ``parallel_compute()`` will revert to serial processing when called within the message passing interface (MPI)\n",
    "     context in a high-performance computing (HPC) cluster. Due to conflicts between MPI, numpy, and joblib, it is\n",
    "     recommended to use a pure MPI approach for computing instead of the MPI + OpenMP (joblib) paradigm.\n",
    "\n",
    "#######################################################################################################################\n",
    " Cleaning up\n",
    " ~~~~~~~~~~~\n",
    " Lets not forget to close and delete the temporarily downloaded file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-10-03T00:35:53.065394Z",
     "iopub.status.busy": "2020-10-03T00:35:53.064642Z",
     "iopub.status.idle": "2020-10-03T00:35:53.069455Z",
     "shell.execute_reply": "2020-10-03T00:35:53.069005Z"
    }
   },
   "outputs": [],
   "source": [
    "h5_file.close()\n",
    "os.remove(h5_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
